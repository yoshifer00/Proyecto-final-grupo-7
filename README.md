DQN Robot Navigation - Grupo 7 (ROS 2 Jazzy)
Este repositorio implementa un entrenamiento de navegaciÃ³n autÃ³noma usando Deep Q-Network (DQN) para un robot mÃ³vil simulado en Stage, utilizando ROS 2 Jazzy.

El objetivo es que el robot aprenda a navegar hacia una meta evitando obstÃ¡culos a partir de informaciÃ³n de LiDAR y odometrÃ­a.

A diferencia de otros proyectos que usan frameworks pesados, aquÃ­ usamos scikit-learn para el agente, lo que facilita mucho la integraciÃ³n directa con los nodos de ROS sin romper dependencias de Python.

ğŸ›  InstalaciÃ³n y Dependencias
Primero, asegÃºrate de tener Ubuntu 24.04 y ROS 2 Jazzy instalado.

1. Paquetes de ROS necesarios
Copia y pega esto en la terminal para instalar lo bÃ¡sico de Stage y mensajes de navegaciÃ³n:

Bash
sudo apt install \
  ros-jazzy-stage-ros \
  ros-jazzy-navigation-msgs \
  ros-jazzy-geometry-msgs \
  ros-jazzy-sensor-msgs \
  ros-jazzy-nav-msgs \
  ros-jazzy-std-srvs
2. Python
Ojo: Estamos usando sklearn para el MLPRegressor.
Bash
sudo apt install python3-sklearn python3-numpy python3-matplotlib
ğŸš€ CÃ³mo ponerlo a correr
ConstrucciÃ³n del Workspace
Clona el repositorio dentro de tu workspace ROS 2:
Bash
mkdir -p ~/proyecto_ws/src
cd ~/proyecto_ws/src
git clone https://github.com/TU_USUARIO/dqn_robot_nav.git
Construye el workspace:
cd ~/proyecto_ws
colcon build
source install/setup.bash

EjecuciÃ³n del entrenamiento
Para que funcione, necesitas dos terminales (no olvides hacer source en ambas):

Terminal 1: El simulador

Bash
ros2 launch stage_ros2 stage.launch.py
AsegÃºrate de que el mundo de Stage estÃ© publicando en /base_scan y /odom, si no, el agente se quedarÃ¡ esperando datos eternamente, lo puedes comprobar en una nueva terminal con
Bash
ros2 topic echo /base_scan
ros2 topic echo /odom

Terminal 2: El entrenamiento

Bash
ros2 launch dqn_robot_nav dqn_training.launch.py
ğŸ§  Detalles del Agente DQN
ğŸ¯ Espacio de acciones

Acciones discretas:

AcciÃ³n	Movimiento
0	Avanzar
1	Girar izquierda
2	Girar derecha
3	Avanzar + izquierda
4	Avanzar + derecha

Las velocidades se pueden ajustar en environment.py.  
Si ves que el robot se queda "atrapado" girando sobre sÃ­ mismo, revisa los parÃ¡metros de recompensa en environment.py.

ğŸ“‚ Estructura del repositorio
dqn_robot_nav/
â”œâ”€â”€ dqn_agent.py # Agente DQN (MLPRegressor + replay + target network)
â”œâ”€â”€ environment.py # Entorno ROS2 (StageEnv)
â”œâ”€â”€ state_processor.py # Procesamiento del estado (LiDAR + goal)
â”œâ”€â”€ train_node.py # Nodo principal de entrenamiento
â”œâ”€â”€ reset_stage.py # Wrapper para reset de Stage y odometrÃ­a
â”œâ”€â”€ launch/
â”‚ â””â”€â”€ dqn_training.launch.py
â””â”€â”€ README.md

ğŸ“ˆ Resultados
Los modelos se guardan automÃ¡ticamente en carpetas con la fecha actual (results_YYYYMMDD_...).

.pkl: El modelo entrenado (pesa poco).

training_results.png: GrÃ¡fica de recompensa por episodio. Ãštil para ver si el robot estÃ¡ aprendiendo algo o si la recompensa estÃ¡ estancada.

ğŸ“ Notas y Errores Comunes
Lag en el Reset: A veces Stage tarda un par de segundos en publicar el primer escaneo tras un reset. El cÃ³digo tiene un pequeÃ±o sleep para evitar errores de puntero nulo.

Rendimiento: Si el entrenamiento va muy lento, cierra la ventana de visualizaciÃ³n de Stage si no la necesitas.

Autores: Daniel Callata, Jhoselin Fernandez, Patricio Flores.
